{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3c732a7",
   "metadata": {},
   "source": [
    "# Assignment 8: Model comparison, Bayesian inference, and random walks\n",
    "\n",
    "## EEB 463"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c84f912",
   "metadata": {},
   "source": [
    "In this assignment, you will explore the statistical behavior of the Ornstein-Uhlenbeck model when using maximum likelihood to infer parameter values from time series data simulated using the approaches introduced in week 5. Start by importing your distributions module (move it into this week's homework directory first)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2e5eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import distributions as dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf00cc42",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "To start, you will need to simulate an Ornstein-Uhlenbeck (OU) walk. We will use the simulated time series generated from this to perform some model comparisons and Bayesian inference. Start by simulating the two-parameter (\"stasis\") OU model from the first part of last week's homework.\n",
    "\n",
    "Replicated text from last week describing the model:\n",
    "\n",
    "We will start by using a simplified version of the OU process. The version we worked with in assignment 5 contained three parameters: &alpha;, &theta;, and &sigma;. To start, we will omit &alpha; (reminder: this represents the strength of the \"pull\" toward the optimum value. As a result, you will need to modify the \"deterministic\" side of the equation used to generate the amount of phenotypic change at each step to omit &alpha;. Under this simplified model, the expression for the expected _deterministic_ change over each time interval, _dt_, given the current value, _y<sub>t</sub>_ is:\n",
    "\n",
    "$$ dX(t) = ( \\theta - y_{t} ) dt $$\n",
    "\n",
    "We will also parameterize the rate (&sigma;) a bit differently from last time. We will assume that the Brownian component of the OU process is evolving with &sigma; = 2. This means that the amount of Brownian change expected over each _dt_ is normally distributed, with standard deviation equal to $\\sigma\\sqrt{dt}$.\n",
    "\n",
    "Simulate a single replicate over 10 million years (you may parameterize time so that 1myr == 1.0), sampling every 100k years, with optimum (&theta;) = 90 and &sigma; = 2. Store the times in a list assigned to the variable `times` and the phenotypes at each time in a list assigned to `pheno`. Import `matplotlib` and plot the time series at the bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ceed89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d2f762d",
   "metadata": {},
   "source": [
    "## Question 1. \n",
    "\n",
    "Implement functions that calculate the negative log-likelihood for 3 different models: simple (single parameter) Brownian motion, the two-parameter Brownian motion with trend model, and the two-parameter OU model. For each model, additionally implement a function to pass to `scipy.optimize.minimize`, like you did last week. Retain the negative log-likelihood at the function minimum found by the optimizer (this will be found in the `.fun` attribute of the object returned by `minimize` and convert it to a (non-negative) log-likelihood. Assign each of these maximized log-likelihoods to the variables `bm_ll`, `trend_ll`, and `stasis_ll`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6515a929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e437003",
   "metadata": {},
   "source": [
    "## Question 2: Model comparison\n",
    "\n",
    "### Q2a.\n",
    "\n",
    "Define functions to calculate the AIC and BIC based on the likelihood, parameter count, and sample size (for BIC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cad9f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65596510",
   "metadata": {},
   "source": [
    "### Q2b.\n",
    "\n",
    "Print the AIC values for each of the three models to the screen. Assign the AIC for each model to variables so that you can use them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce21ce1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "988fde5c",
   "metadata": {},
   "source": [
    "### Q2c.\n",
    "\n",
    "Print the BIC values for each of the three models to the screen. Assign the BIC for each model to variables so that you can use them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86086463",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "417bd2dc",
   "metadata": {},
   "source": [
    "### Q2d.\n",
    "\n",
    "Calculate AIC and BIC weights for each model. Print the weight for each model and each information criterion to the screen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be661f6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e115bd14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b70764d",
   "metadata": {},
   "source": [
    "### Q2e.\n",
    "\n",
    "Do AIC and BIC favor the same model? Is a single model strongly supported (AIC weight > 0.95), or is there more uncertainty? Is the generating (OU) model favored? If not, speculate on why it is not. Next, rerun all the cells above this one. Do you achieve the same result? Lastly, synthesize the results described above to explain your own confidence in the ability to reconstruct historical dynamics from time series data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee1c3e8",
   "metadata": {},
   "source": [
    "TYPE ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561fc15e",
   "metadata": {},
   "source": [
    "## Question 2. Bayesian inference and priors\n",
    "\n",
    "### Q2a.\n",
    "\n",
    "Let's add &alpha; back into our OU model. Simulate a single replicate of an OU process with this slightly more complex model by modifying the code from question 1a. Store the times in `times` and the simulated phenotypes at each time in the variable `pheno`. Use the same parameter values for &theta; and &sigma; as you used in Question 1, and set &alpha; to equal 2.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a605d407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6bd77aa",
   "metadata": {},
   "source": [
    "### Q2b.\n",
    "\n",
    "Define your log-likelihood function (you can use the one from last week or complete the declarations started below). Below this, implement a function to perform MCMC to estimate the posterior probability distribution for this three paramter model. You may copy, paste, and modify the `stasis_MCMC` function introduced in class, adding additional arguments for the alpha prior and proposal distributions. Remember that, like in class, you will need to define a separate function to draw proposal values to alter any paramters that are bounded at zero (remember we did this for &sigma; to ensure we never propose values <0). You can use the same function for both &alpha; and &sigma;."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23461cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ornstein_ts_ll(alpha, theta, sigma, times, data):\n",
    "    return\n",
    "\n",
    "def generate_positive_proposal(param, prop_dist):\n",
    "    return \n",
    "\n",
    "\n",
    "def OU_MCMC(gen,thin,start_p,alpha_prior,theta_prior,sigma_prior,alpha_prop,theta_prop,sigma_prop):\n",
    "    return\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0fb2f9",
   "metadata": {},
   "source": [
    "### Q2c.\n",
    "\n",
    "Run the Markov chain for 100,000 generations sampling every 100 generations. Use the starting values, proposals, and priors provided below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0ee2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha, theta, sigma = 1.8, 80., 1.3\n",
    "alpha_prior = dist.normal(alpha, 0.1)\n",
    "theta_prior = dist.normal(theta, 5.)\n",
    "sigma_prior = dist.normal(sigma, 0.25)\n",
    "alpha_prop = dist.normal(0.0, 0.1)\n",
    "theta_prop = dist.normal(0.0, 2.)\n",
    "sigma_prop = dist.normal(0., 0.4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25e2193",
   "metadata": {},
   "source": [
    "## Question 3. Analyzing our samples\n",
    "\n",
    "### Q3a. \n",
    "\n",
    "Discard the first 10% of samples for each parameter and assign each resulting list of samples to a new variable so that you have three separate variables-- one for each parameter, each containing the last 90% of samples. For plotting, you also may wish to create a separate variable storing the list of generations (this will be your x-axis for the next question)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e09a95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21862321",
   "metadata": {},
   "source": [
    "### Q3b. \n",
    "\n",
    "Plot the post-burnin chains for all three parameters. How do they look? Does the Markov chain appear to have reached its stationary distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6369cfc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c89117d0",
   "metadata": {},
   "source": [
    "### Q3c.\n",
    "\n",
    "Plot histograms of the sampled values for all three parameters. Are the average and range of values across the posterior distribution close to the true values? Compare the closeness of these paramter estimates to those that we arrived at under maximum likelihood for the 3-paramter OU model last week. Are mean posterior values closer than the MLEs from last week?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60dfdd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2157120",
   "metadata": {},
   "source": [
    "### Q3d. \n",
    "\n",
    "LAST QUESTION.\n",
    "\n",
    "Import seaborn and create a jointplot (like we did in lecture) to visualize the joint posterior for &alpha; and &theta;. Compare this joint posterior to the likelihood surface for these two parameters from last week. The posterior should have a clearer peak. Why do you suppose this is? Is MCMC extracting more information from the data than optimization under maximum likelihood? Or could something else explain this disparity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb4205d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
