{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c73e892",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# week 7: statistical inference \n",
    "\n",
    "dr. tomomi parins-fukuchi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2aec02",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### what is inference?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084b878e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### what is inference?\n",
    "\n",
    "- **statistical inference:** making a proposition about some properties of a population\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d762b74",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### what is inference?\n",
    "\n",
    "- **statistical inference:** making a proposition about some properties of a population\n",
    "\n",
    "- this can help us make generalizations to learn how the world works\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51535937",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### what is inference?\n",
    "\n",
    "\n",
    "- **statistical inference:** making a proposition about some properties of a population\n",
    "\n",
    "- this can help us make generalizations to learn how the world works\n",
    "\n",
    "- this process involves both **data** and a **model**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f201783",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### models and data \n",
    "\n",
    "- so far in this course, we have learned about several important models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417591dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### models and data \n",
    "\n",
    "- so far in this course, we have learned about several important models\n",
    "\n",
    "- we have focused on **forward simulation**:\n",
    "  - what is the range of patterns we see when we generate data under a model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f79e708",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### models and data \n",
    "\n",
    "- so far in this course, we have learned about several important models\n",
    "\n",
    "- we have focused on **forward simulation**:\n",
    "  - what is the range of patterns we see when we generate data under a model?\n",
    "  \n",
    "- now we will ask about **model-based inference**: \n",
    "  - how can stochastic models be used to interrogate patterns in empirical data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dff534",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### statistical paradigms\n",
    "\n",
    "- frequentism\n",
    "- bayesian\n",
    "- \"likelihoodist\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2871c2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### frequentism\n",
    "\n",
    "- all the classics you learned in Stats 101 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3192ad8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### frequentism\n",
    "\n",
    "- all the classics you learned in Stats 101 \n",
    "- probability = the proportion of outcomes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f8f390",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### frequentism\n",
    "\n",
    "- all the classics you learned in Stats 101 \n",
    "- probability = the proportion of outcomes\n",
    "    - P(event) = the % of times it happens over many trials\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61887163",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### frequentism\n",
    "\n",
    "- all the classics you learned in stats 101 \n",
    "- probability = the proportion of outcomes\n",
    "    - P(event) = the % of times it happens over many trials\n",
    "- uncertainty in the properties we infer are the result of statistical sampling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742da2f2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### bayesianism\n",
    "\n",
    "- what we can learn from data is influenced by our prior beliefs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586168ff",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### bayesianism\n",
    "\n",
    "- what we can learn from data is influenced by our prior beliefs\n",
    "- probabilities reflect degree of belief after seeing some data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a21060b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### bayesianism\n",
    "\n",
    "- what we can learn from data is influenced by our prior beliefs\n",
    "- probabilities reflect degree of belief after seeing some data\n",
    "- uncertainty in the properties we infer reflect differences in degree of belief "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea78e80",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### bayesianism\n",
    "\n",
    "- what we can learn from data is influenced by our prior beliefs\n",
    "- probabilities reflect degree of belief after seeing some data\n",
    "- uncertainty in the properties we infer reflect differences in degree of belief \n",
    "- we will talk in more detail about this next week"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26565b9f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### likelihood function\n",
    "\n",
    "$$P(data|model)$$\n",
    "\n",
    "- a unifying construct across paradigms\n",
    "- mathematical function specifying the probability of an observation under a model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7f9800",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### likelihood function\n",
    "\n",
    "- can often use the pdf or pmf of a distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac192f1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### likelihood function\n",
    "\n",
    "![](images/norm.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88b2387",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### likelihood function\n",
    "\n",
    "$$ f(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi} e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^2}}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79433207",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### likelihood function\n",
    "\n",
    "$$ f(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi} e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^2}}$$ \n",
    "\n",
    "- Density of $x$ given parameters $\\mu$ (mean) and $\\sigma$ (standard deviation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0081b083",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### likelihood function\n",
    "\n",
    "$$ f(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi} e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^2}}$$ \n",
    "\n",
    "- Density of $x$ given parameters $\\mu$ (mean) and $\\sigma$ (standard deviation)\n",
    "\n",
    "- I.e., gives (probability) density of an observation ($x$) given model and parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbfd6b3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### what is the probability that a coin is 'fair'*?\n",
    "\n",
    "*has an equal chance of being heads or tails when flipped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cb8c6b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Binomial PMF\n",
    "\n",
    "$$ P(x|p,n) = \\binom{n}{x} p^x q^{n-x}$$\n",
    "\n",
    "p = probability of success in one trial\n",
    "\n",
    "q = probability of failure in one trial\n",
    "\n",
    "x = number of times that a specific outcome occurs in n trials\n",
    "\n",
    "n = number of trials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eabacef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Binomial PMF and fair coins\n",
    "\n",
    "$$ P(x) = \\binom{n}{x} p^x q^{n-x}$$\n",
    "\n",
    "p = probability of **heads** in one trial\n",
    "\n",
    "q = probability of **tails** in one trial\n",
    "\n",
    "x = number of times you observe a heads\n",
    "\n",
    "n = number of flips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdec964b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Binomial PMF and fair coins\n",
    "\n",
    "So if we assume a fair coin...\n",
    "\n",
    "$$ P(x) = \\binom{n}{x} 0.5^x 0.5^{n-x}$$\n",
    "\n",
    "This expression is equivalent to asking the probability of observing x heads in n flips, assuming the coin is fair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ca2225",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from math import factorial\n",
    "\n",
    "def binom_prob_fair(x, n):\n",
    "    nchoosex = factorial(n) / ( factorial(x) * factorial(n - x) )\n",
    "    prob = nchoosex * (0.5 ** float(x)) * (0.5 ** float(n-x))\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966ce4e4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### what is the probability that a coin is 'fair'*?\n",
    "\n",
    "- we will create a binomial likelihood function to compute the likelihood of a dataset of coin flip trials\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43af6556",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### what is the probability that a coin is 'fair'*?\n",
    "\n",
    "- we will create a binomial likelihood function to compute the likelihood of a dataset of coin flip trials\n",
    "\n",
    "- this will tell us how likely we are to see data we generated experimentally if the coin is fair\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1855ef6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Binomial PMF\n",
    "\n",
    "$$ P(x) = \\binom{n}{x} p^x q^{n-x}$$\n",
    "\n",
    "p = probability of success in one trial\n",
    "\n",
    "q = probability of failure in one trial\n",
    "\n",
    "x = number of times that a specific outcome occurs in n trials\n",
    "\n",
    "n = number of trials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf675e5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Binomial PMF and fair coins\n",
    "\n",
    "$$ P(x) = \\binom{n}{x} p^x q^{n-x}$$\n",
    "\n",
    "p = probability of **heads** in one trial\n",
    "\n",
    "q = probability of **tails** in one trial\n",
    "\n",
    "x = number of times you observe a heads\n",
    "\n",
    "n = number of flips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ef5ab7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Binomial PMF and fair coins\n",
    "\n",
    "So if we assume a fair coin...\n",
    "\n",
    "$$ P(x) = \\binom{n}{x} 0.5^x 0.5^{n-x}$$\n",
    "\n",
    "This expression is equivalent to asking:\n",
    "\n",
    "$$ P(x | fair) $$\n",
    "\n",
    "Or, the probability of observing x heads in n flips, assuming the coin is fair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98a7e53",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# what it the probability of observing 7 heads in 10 flips given a model where the coin is fair?\n",
    "\n",
    "import distributions as dist\n",
    "\n",
    "binom = dist.binomial(n = 10, p = 0.5)\n",
    "print(binom.pmf(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9eb9f4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# what is the probability of observing x heads in 10 flips given a model where the coin is fair?\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "binom = dist.binomial(n = 10, p = 0.5)\n",
    "x = []\n",
    "probs = []\n",
    "\n",
    "\n",
    "for i in range(11):\n",
    "    x.append(i)\n",
    "    p = binom.pmf(i)\n",
    "    probs.append(p)\n",
    "    \n",
    "plt.plot(x,probs,\"o\")\n",
    "plt.xlabel(\"number of heads\")\n",
    "plt.ylabel(\"probability\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40814a46",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## likelihood\n",
    "\n",
    "- okay now let's dispense with the assumption that the coin is fair\n",
    "- let's again say we observe 7 heads in 10 tosses:\n",
    "    - **what is the degree of bias that would best explain this observation?**\n",
    "    - this is called _maximizing the likelihood_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad9c3b6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# we will take our \"dataset\" of 7 heads and see how the likelihood changes as we change the 'p' parameter\n",
    "\n",
    "p_heads = 0.05\n",
    "binom = dist.binomial(n = 10, p = p_heads)\n",
    "\n",
    "x = []\n",
    "l = []\n",
    "\n",
    "while p_heads < 1.0:\n",
    "    prob = binom.pmf(7)\n",
    "    x.append(p_heads)\n",
    "    l.append(prob)\n",
    "    p_heads += 0.05\n",
    "    binom.p = p_heads\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a7d39b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(x,l,\"o\")\n",
    "plt.xlabel(\"prob. heads\")\n",
    "plt.ylabel(\"likelihood\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42712a6d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### maximum likelihood\n",
    "\n",
    "- find the set of parameters that yields highest likelihood for a dataset\n",
    "- parameters with the highest likelihood called \"maximum likelihood estimates\" (MLEs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3064b39",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### maximum likelihood\n",
    "\n",
    "- model SVL measurements for a population of lizards using a normal distribution\n",
    "- **what is the value of the mean that maximizes the probability density of the data?**\n",
    "\n",
    "$$ L = f(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi} e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^2}}$$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab373e6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import distributions as dist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "svl = dist.normal(130.0,10.0).sample(5)\n",
    "plt.hist(svl)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9a7c8c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## maximum likelihood\n",
    "\n",
    "likelihood of the svl data:\n",
    "\n",
    "$$ P(measurements | model, parameters ) $$\n",
    "\n",
    "or:\n",
    "\n",
    "$$ P(x | N(\\mu,\\sigma) ) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3df51a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(svl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c81928",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## maximum likelihood\n",
    "\n",
    "- we will assume each svl measurement is independent and drawn from the same distribution as all others\n",
    "- likelihood of the full dataset can be calculated as product of the likelihood of each measurement:\n",
    "\n",
    "$$ P(svl | N(\\mu,\\sigma) = P(svl_1 | N(\\mu,\\sigma)) * P(svl_2 | N(\\mu,\\sigma))...  P(svl_N | N(\\mu,\\sigma))  $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345e04f9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## log likelihood\n",
    "\n",
    "- multiplying each independent likelihood many times can lead to very small numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ac4e58",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "0.05 * 0.05 * 0.05 * 0.05 * 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dc1dda",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## log-likelihood\n",
    "\n",
    "- the mathematical properties of logarithms can help avoid issues with this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69640ba1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "math.log(.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001780a4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## log-likelihood\n",
    "\n",
    "- the mathematical properties of logarithms can help avoid issues with this\n",
    "\n",
    "- so we typically sum independent **log-likelihood** values instead of multiplying independent likelihoods\n",
    "\n",
    "$$ \\log x * y = \\log x + \\log y $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddab5f7e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(math.log(0.05 * 0.05 * 0.05 * 0.05 * 0.05))\n",
    "print(math.log(0.05) + math.log(0.05) + math.log(0.05)+ math.log(0.05) + math.log(0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f86982",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## likelihood surface\n",
    "\n",
    "- we often want to find the value of a parameter that maximizes the (log) likelihood\n",
    "    - e.g., we may wish to estimate the mean svl length of the population that generated our svl samples\n",
    "    - what value for the mean parameter provides the strongest explanation for our data?\n",
    "        - we call this value the **maximum likelihood estimate (MLE)**\n",
    "    - one approach is to compute the likelihood of a bunch of different guesses for the value of the parameter\n",
    "        - we call this the **likelihood surface**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1c3049",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "mean_guess = 110.0\n",
    "norm = dist.normal(mean_guess, 10.0)\n",
    "\n",
    "x = []\n",
    "ll = []\n",
    "for _ in range(40):\n",
    "    log_like = 0.0\n",
    "    for i in svl:\n",
    "        log_like += norm.log_pdf(i)\n",
    "    x.append(mean_guess)\n",
    "    ll.append(log_like)\n",
    "    mean_guess += 1.0\n",
    "    norm.mean = mean_guess\n",
    "    \n",
    "plt.plot(x,ll)\n",
    "plt.xlabel(\"mean guess\")\n",
    "plt.ylabel(\"log-likelihood\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32f0fa7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### maximum likelihood\n",
    "\n",
    "- MLEs can sometimes be derived analytically for a given model\n",
    "    - i.e., an exact mathematical solution can be found for the value at the \"peak\" of the likelihood surface\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc362db",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### maximum likelihood\n",
    "\n",
    "- the general approach to doing this involves: \n",
    "  1) taking the partial derivative of the likelihood function with respect to each parameter\n",
    "  2) setting the result to zero\n",
    "  3) solving for the parameter\n",
    "\n",
    "$$ f(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi} e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^2}}$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb7620c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### maximum likelihood\n",
    "\n",
    "- For a normal distribution, the result for each parameter looks familiar:\n",
    "\n",
    "$$ \\hat{\\mu} = \\frac{\\sum_{i=1}^n x_i}{n} $$\n",
    "\n",
    "$$ \\hat{\\sigma} =  \\sqrt{ \\frac{ \\sum_{i=1}^n  (x_1 - \\hat{\\mu} ) }{n}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33064ae5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### maximum likelihood\n",
    "\n",
    "- analytical derivation of MLEs works for simple models\n",
    "- more complex models are often impossible\n",
    "- one common solution is **numerical optimization** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e0b934",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## numerical optimization\n",
    "\n",
    "\n",
    "- highly optimized set of rules to \"plug and chug\" different parameter values\n",
    "- we want to traverse the likelihood surface and claw our way to the highest peak we can find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfb07c6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "mean_guess = 110.0\n",
    "norm = dist.normal(mean_guess, 10.0)\n",
    "\n",
    "x = []\n",
    "ll = []\n",
    "for _ in range(40):\n",
    "    log_like = 0.0\n",
    "    for i in svl:\n",
    "        log_like += norm.log_pdf(i)\n",
    "    x.append(mean_guess)\n",
    "    ll.append(log_like)\n",
    "    mean_guess += 1.0\n",
    "    norm.mean = mean_guess\n",
    "    \n",
    "plt.plot(x,ll)\n",
    "plt.xlabel(\"mean guess\")\n",
    "plt.ylabel(\"log-likelihood\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bef6df",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## minimization\n",
    "\n",
    "- it is convention to seek the minimum of a function rather than the maximum\n",
    "- we can make our log-likelihood approach fit by **minimizing the _negative_ log-likelihood**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b112657",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "mean = 110.0\n",
    "norm = dist.normal(mean,10.0)\n",
    "\n",
    "x = []\n",
    "ll = []\n",
    "for _ in range(40):\n",
    "    log_like = 0.0\n",
    "    for i in svl:\n",
    "        log_like += norm.log_pdf(i)\n",
    "    x.append(mean)\n",
    "    ll.append(-log_like)\n",
    "    mean += 1.0\n",
    "    norm.mean = mean \n",
    "    \n",
    "plt.plot(x,ll)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d42b95",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### numerical optimization\n",
    "\n",
    "\n",
    "- highly optimized set of rules to \"plug and chug\" different parameter values\n",
    "- often involves calculating or approximating the first and/or second derivative of the likelihood function to decide which direction to go\n",
    "    - e.g., if derivative is negative, move _x_ to the right, if positive, move to the left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5fff1b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(x,ll)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5eddcb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## numerical optimization\n",
    "\n",
    "many algorithms exist, each requiring different information\n",
    "\n",
    "![bg right h:700](images/numerical_recipes.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9fa08b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## numerical optimization\n",
    "\n",
    "\n",
    "- the `scipy` module implements many numerical optimization routines\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57bac91",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy import optimize\n",
    "\n",
    "def norm_neg_ll(mean, sd, x):\n",
    "    dens = math.log((1.0/(sd*math.sqrt(2.*math.pi))))+((-.5 * (((x - mean)/sd)**2.)))\n",
    "    return -dens\n",
    "\n",
    "def evaluate_norm(params, sd, measurements):\n",
    "    mean = params[0]\n",
    "    if mean <= 0.0:\n",
    "        return 10000000\n",
    "    neg_log_like = 0.0\n",
    "    for i in measurements:\n",
    "        neg_log_like += norm_neg_ll(mean, sd, i)\n",
    "    return neg_log_like\n",
    "\n",
    "# simulate 5 measurements from normal distribution with mean 130 and sd = 10\n",
    "svl = dist.normal(130.0,10.0).sample(5)\n",
    "res = optimize.minimize(evaluate_norm, [100.], args=(10.0, svl), method = \"Nelder-Mead\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2121c7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## numerical optimization\n",
    "\n",
    "- let's plot the MLE relative to the likelihood surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccc0361",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.linspace(110.,150.,50)\n",
    "ll = [evaluate_norm([mean], 10.0 ,svl) for mean in x]\n",
    "\n",
    "plt.plot(x,ll)\n",
    "plt.plot(res.x[0],res.fun,\"o\")\n",
    "plt.xlabel(\"mean estimate\")\n",
    "plt.ylabel(\"negative log-likelihood\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb99e941",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## sample size and MLEs\n",
    "\n",
    "we can se how the size of our dataset impacts our MLEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff218be1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "svl = dist.normal(130.0,10.0).sample(5)\n",
    "res = optimize.minimize(evaluate_norm, [120.], args=(10.0, svl), method = \"Nelder-Mead\")\n",
    "\n",
    "x = np.linspace(110.,150.,50)\n",
    "ll = [evaluate_norm([mean], 10.0 ,svl) for mean in x]\n",
    "\n",
    "    \n",
    "plt.plot(x,ll)\n",
    "plt.plot(res.x[0],res.fun,\"o\")\n",
    "plt.xlabel(\"mean estimate\")\n",
    "plt.ylabel(\"negative log-likelihood\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeac8578",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## optimizing multiple parameters\n",
    "\n",
    "- what if our model has multiple unknown parameters?\n",
    "- this can become a complex problem, because value of one parameter may influence the likelhood of another parameter\n",
    "- most optimization routines are designed to handle multidimensional problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96e138e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_norm_2param(params, measurements):\n",
    "    mean = params[0]\n",
    "    sd = params[1]\n",
    "    if mean <= 0.0:\n",
    "        return 10000000\n",
    "    neg_log_like = 0.0\n",
    "    for i in measurements:\n",
    "        neg_log_like += norm_neg_ll(mean, sd, i)\n",
    "    return neg_log_like\n",
    "\n",
    "svl = dist.normal(130.0, 10.0).sample(10) # simulate measurements\n",
    "\n",
    "res = optimize.minimize(evaluate_norm_2param, [100., 1.0], args=(svl), method = \"Nelder-Mead\")\n",
    "print(\"mean:\",res.x[0],\"\\nsd:\",res.x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e687a1c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X = np.linspace(110, 150, 100)\n",
    "Y = np.linspace(res.x[1] - (res.x[1] / 2.), res.x[1] + (res.x[1] / 2.), 100)\n",
    "x, y = np.meshgrid(X, Y)\n",
    "\n",
    "z = []\n",
    "\n",
    "for yi in Y: \n",
    "    y_ll = []\n",
    "    for xi in X:\n",
    "        ll = -evaluate_norm_2param([xi,yi], svl)\n",
    "        y_ll.append(ll)\n",
    "        \n",
    "    z.append(y_ll)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235166e3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "levels = sorted(list(set([-(int(res.fun)+70.),-(int(res.fun)+50.),-(int(res.fun)+25.),-(int(res.fun)+10.), -(int(res.fun)+5.),-(int(res.fun)+1.),-(int(res.fun)+.2)])))\n",
    "# print(levels)\n",
    "fig, ax = plt.subplots()\n",
    "CS = ax.contour(x,y,z,levels=levels)\n",
    "ax.clabel(CS, inline=True, fontsize=10)\n",
    "ax.plot(res.x[0],res.x[1],\"o\",color=\"black\")\n",
    "plt.xlabel(\"mean estimate\")\n",
    "plt.ylabel(\"sd estimate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ae9581",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## inferring model parameters using maximum likelihood\n",
    "\n",
    "- use optimization to find parameter values that maximize prob. of observing dataset\n",
    "- what value of &sigma; maximizes the probability of observing a time series under Brownian motion?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea474f3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## brownian motion\n",
    "\n",
    "movement of particle X over time interval t (_dX(t)_) is a normally distributed rv\n",
    "\n",
    "$$ dX(t) \\sim N(0,\\sigma\\sqrt{t}) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ae0c62",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# simulate our data\n",
    "\n",
    "time_step = 0.1   # we will sample the process at every dt = 0.1 ma or 100k years\n",
    "total_myr = 10.0\n",
    "n_iter = int(total_myr / time_step)\n",
    "\n",
    "curtime = 0.0\n",
    "x = []\n",
    "y_t = 100.0\n",
    "y = []\n",
    "\n",
    "sigma = 0.5  # rate parameter for BM\n",
    "norm = dist.normal(mean=0.0, sd = sigma)\n",
    "for i in range(n_iter-1):\n",
    "    y.append(y_t)\n",
    "    x.append(curtime)\n",
    "    \n",
    "    norm.sd = sigma * math.sqrt(time_step)\n",
    "    delta_y = norm.sample(1)[0]\n",
    "    y_t += delta_y\n",
    "    curtime = curtime + time_step\n",
    "        \n",
    "plt.plot(x,y)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae00948",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## maximum likelihood and brownian motion\n",
    "\n",
    "- we can calculate the likelihood for a particular time series\n",
    "  - we want to calculate the likelihood of each change in phenotype, given $\\sigma$ and _dt_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c064999",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## the recipe\n",
    "\n",
    "- loop over time series and calculate difference between each i and i-1\n",
    "    - call it delta_y\n",
    "- calculate the likelihood of delta_y given $\\sigma$ and $dt$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb81e4f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def brownian_ts_negll(sigma, times, data):\n",
    "    ll = 0.0\n",
    "    for i in range(1,len(times)):\n",
    "        cur_val = data[i]\n",
    "        last_val = data[i-1]\n",
    "        delta_y = last_val - cur_val\n",
    "        delta_t = times[i] - times[i-1]\n",
    "        sd = sigma * math.sqrt(delta_t)\n",
    "        stepll = math.log((1.0 / math.sqrt(2.0 * math.pi * (sd ** 2.0)))) + (-((delta_y - 0.0)**2.0) / (2.0 * sd ** 2.0))\n",
    "        ll += stepll\n",
    "    return -ll\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfeda63",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## maximum likelihood and brownian motion\n",
    "\n",
    "- we can calculate the likelihood for a particular time series\n",
    "- optimize &sigma; param using `scipy.minimize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d390f6a3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_brownian(params,times,data):\n",
    "    sigma = params[0]\n",
    "    if sigma <= 0.0:\n",
    "        return 100000000.\n",
    "    \n",
    "    nll = brownian_ts_negll(sigma,times,data)\n",
    "    return nll\n",
    "        \n",
    "\n",
    "res = optimize.minimize(evaluate_brownian, [1.], args=(x, y), method = \"Powell\")\n",
    "rate_mle = res.x[0]\n",
    "print(\"sigma MLE:\",rate_mle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8659eb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## maximum likelihood and brownian motion\n",
    "\n",
    "we can plot the likelihood surface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab21c302",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "rates = np.linspace(0.05,2.,50)\n",
    "surf = [evaluate_brownian([rate], x, y) for rate in rates]\n",
    "\n",
    "plt.plot(rates,surf)\n",
    "plt.plot(rate_mle,res.fun,\"o\",color=\"black\")\n",
    "plt.xlabel(\"sigma\")\n",
    "plt.ylabel(\"neg. log-likelihood\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f6465b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## maximum likelihood and brownian motion\n",
    "\n",
    "- we can also optimize multiple parameters at a time\n",
    "- this is useful for more complex models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9315c8f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# generate data for Brownian model with positive trend\n",
    "\n",
    "time_step = 0.1\n",
    "total_myr = 20.0\n",
    "n_iter = int(total_myr / time_step)\n",
    "\n",
    "curtime = 0.0\n",
    "times = []\n",
    "y_t = 100.0\n",
    "pheno = []\n",
    "\n",
    "trend = 2.8\n",
    "sigma = 1.5\n",
    "norm = dist.normal(mean=0.0, sd = sigma)\n",
    "for i in range(n_iter-1):\n",
    "    pheno.append(y_t)\n",
    "    times.append(curtime)\n",
    "\n",
    "    delta_y = (norm.sample(1)[0] + (trend * time_step)) * math.sqrt(time_step)\n",
    "    y_t += delta_y\n",
    "    curtime = curtime + time_step\n",
    "        \n",
    "plt.plot(times,pheno)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7095cc47",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def trend_ts_negll(trend, sigma, times, data):\n",
    "    ll = 0.0\n",
    "    norm = dist.normal(trend,1.0)\n",
    "\n",
    "    for i in range(1,len(times)):\n",
    "        cur_val = data[i]\n",
    "        last_val = data[i-1]\n",
    "        delta_y = last_val - cur_val\n",
    "        delta_t = times[i] - times[i-1]\n",
    "        mean = trend * delta_t\n",
    "        sd = sigma * math.sqrt( delta_t)\n",
    "        stepll = math.log((1.0/(sd*math.sqrt(2.*math.pi)))) + ((-.5 * (((delta_y - mean) / sd ) ** 2.)))\n",
    "        ll += stepll\n",
    "    return -ll\n",
    "\n",
    "def evaluate_trend(params,times,data):\n",
    "    trend = params[0]\n",
    "    sigma = params[1]\n",
    "    if sigma <= 0.0:\n",
    "        return 100000000.\n",
    "    \n",
    "    nll = trend_ts_negll(trend,sigma,times,data)\n",
    "    return nll\n",
    "\n",
    "\n",
    "    \n",
    "res = optimize.minimize(evaluate_trend, [0.0, 1.0], args=(times, pheno), method = \"BFGS\")\n",
    "trend_mle = res.x[0]\n",
    "rate_mle = res.x[1]\n",
    "print(\"trend MLE:\",trend_mle,\"\\nsigma MLE:\",rate_mle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe1b920",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "trends = np.linspace(-.7,2.,100)\n",
    "surf = [evaluate_trend([trend, rate_mle], times, pheno) for trend in trends]\n",
    "\n",
    "\n",
    "plt.plot(trends,surf)\n",
    "plt.plot(trend_mle,res.fun,\"o\",color=\"black\")\n",
    "plt.xlabel(\"trend\")\n",
    "plt.ylabel(\"neg. log-likelihood\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ba8f57",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X = np.linspace(res.x[0] - (res.x[0] / 2.), res.x[0] + (res.x[0] / 2.), 100)\n",
    "Y = np.linspace(res.x[1] - (res.x[1] / 2.), res.x[1] + (res.x[1] / 2.), 100)\n",
    "Xgrid, Ygrid = np.meshgrid(X, Y)\n",
    "z = []\n",
    "\n",
    "for yi in Y: \n",
    "    y_ll = []\n",
    "    for xi in X:\n",
    "        ll = -evaluate_trend([xi,yi], times, pheno)\n",
    "        y_ll.append(ll)\n",
    "        \n",
    "    z.append(y_ll)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527fa438",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "levels = [-res.fun - 20,-res.fun - 10,-res.fun - 5,-res.fun - 1, -res.fun - .1] \n",
    "fig, ax = plt.subplots()\n",
    "CS = ax.contour(Xgrid,Ygrid,z,levels=levels)\n",
    "ax.clabel(CS, inline=True, fontsize=10)\n",
    "ax.plot(res.x[0],res.x[1],\"o\",color=\"black\")\n",
    "plt.xlabel(\"trend MLE\")\n",
    "plt.ylabel(\"rate MLE\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
