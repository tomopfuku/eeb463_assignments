{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3c732a7",
   "metadata": {},
   "source": [
    "# Assignment 7: Maximum likelihood, identifiability, and Ornstein-Uhlenbeck\n",
    "\n",
    "## EEB 463"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c84f912",
   "metadata": {},
   "source": [
    "In this assignment, you will explore the statistical behavior of the Ornstein-Uhlenbeck model when using maximum likelihood to infer parameter values from time series data simulated using the approaches introduced in week 5. Start by importing your distributions module (move it into this week's homework directory first)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2e5eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import distributions as dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf00cc42",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "### Q1a. \n",
    "\n",
    "Here, you will simulate an Ornstein-Uhlenbeck (OU) walk. You can start with the code that you wrote in question 4 of assignment 5. We can think of the simulated data for this assignment as stemming from the same motivating biological conditions as that question. \n",
    "\n",
    "We will start by using a simplified version of the OU process. The version we worked with in assignment 5 contained three parameters: &alpha;, &theta;, and &sigma;. To start, we will omit &alpha; (reminder: this represents the strength of the \"pull\" toward the optimum value. As a result, you will need to modify the \"deterministic\" side of the equation used to generate the amount of phenotypic change at each step to omit &alpha;. Under this simplified model, the expression for the expected _deterministic_ change over each time interval, _dt_, given the current value, _y<sub>t</sub>_ is:\n",
    "\n",
    "$$ dX(t) = ( \\theta - y_{t} ) dt $$\n",
    "\n",
    "We will also parameterize the rate (&sigma;) a bit differently from last time. We will assume that the Brownian component of the OU process is evolving with &sigma; = 2. This means that the amount of Brownian change expected over each _dt_ is normally distributed, with standard deviation equal to $\\sigma\\sqrt{dt}$.\n",
    "\n",
    "Simulate a single replicate over 10 million years (you may parameterize time so that 1myr == 1.0), sampling every 100k years, with optimum (&theta;) = 90 and &sigma; = 2. Store the times in a list assigned to the variable `times` and the phenotypes at each time in a list assigned to `pheno`. Import `matplotlib` and plot the time series at the bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ceed89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d2f762d",
   "metadata": {},
   "source": [
    "### Q1b. \n",
    "\n",
    "We will attempt to infer the maximum likelihood estimates (MLEs) for &theta; and &sigma; using the minimization routines in `scipy.optimize`. Import this module. We will be using the function `minimize`. This requires as input the function used to calculate the negative log-likelihood, a list that contains the starting values of each parameter you will optimize (in this case, &theta; and &sigma;), and a tuple containing each of the additional arguments needed for the function. \n",
    "\n",
    "Before optimizing, you will need to define a function to evaluate the likelihood at each iteration of the minimization routine. We will call the current version of our OU model a \"stasis\" model, after Hunt (2006). Accordingly, define a function called `stasis_ts_negll` that takes parameters `theta`, `sigma`, `times`, `pheno`. Within this function, loop over `pheno` (which contains the simulated measurements), and calculate the log-likelihood of the change over each time interval (the phenotypic change between _t_ = i - 1 and _t_ = i). _Remember that the likelihood of a series of independent observations is their product, and so their log-likelihood is simply their sum._ \n",
    "\n",
    "HINT: To derive the log-likelihood function, remember how we generated each phenotypic change in the previous question-- we used a normal distribution with mean equal to the deterministic OU term and standard deviation equal to &sigma; $\\sqrt{dt}$. Make sure that you calculate the **log** density from this distribution. \n",
    "\n",
    "HINT 2: **Remember to return the NEGATIVE log-likelihood!!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6515a929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a3abe56",
   "metadata": {},
   "source": [
    "### Q1c. \n",
    "\n",
    "Complete the function below to take arguments `params`, which will be a list containing the parameters you wish to optimize, `times`, and `pheno`. Since under our model, &sigma;<sup>2</sup> cannot be equal to or less than zero, return an arbitrarily large value if the &sigma;<sup>2</sup> value passed via `params` falls <= zero. Pass the parameters and data to `stasis_ts_negll` and return the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f166ebd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e437003",
   "metadata": {},
   "source": [
    "### Q1d.\n",
    "\n",
    "Use starting values &theta; = 50, &sigma; = 0.5, run `optimize.minimize`. You can try any of the methods that don't require that you impute gradient information. Some of these include \"Nelder-Mead\", \"Powell\", and \"BFGS\". Assign the output from the minimization procedure to the variable `res`. Print out the MLEs for each parameter ($\\hat{\\theta}$; and $\\hat{\\sigma}$). How close are these to the true, simulated, values?\n",
    "\n",
    "HINT: the MLEs are contained within attribute `x` of `res` (i.e., access by typing `res.x`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cad9f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65596510",
   "metadata": {},
   "source": [
    "### Q1e. \n",
    "\n",
    "Plot the negative log-likelihood surface for the optimum, &theta; over the interval spanning $\\hat{\\theta} - \\frac{\\hat{\\theta}}{4} $ and $\\hat{\\theta} + \\frac{\\hat{\\theta}}{4} $, when &sigma; is held constant at the MLE. Label the axes. Does the area surrounding $\\hat{\\theta}$ have a clearly defined minimum? Do you think there is strong statistical evidence for the MLE?\n",
    "\n",
    "HINT: you might use `np.linspace` to generate evenly spaced &theta; values, for each of which you will calculate the negative log-likelihood. You can also iterate manually over evenly spaced &theta; values if you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86086463",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "417bd2dc",
   "metadata": {},
   "source": [
    "### Q1f.\n",
    "\n",
    "Borrow and alter the code from lecture to visualize the 2-dimensional likelihood surface for &theta; and &sigma; using a contour plot. You might convert the negative log-likelihoods to positive log-likelihoods like we did in class. Label the axes. Is there a clearly defined peak in the contour plot? Does this mean there is strong or weak evidence from which to 'identify' which values for &theta; and &sigma; best explain the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e115bd14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "561fc15e",
   "metadata": {},
   "source": [
    "## Question 2.\n",
    "\n",
    "### Q2a.\n",
    "\n",
    "Let's add &alpha; back into our OU model. Simulate a single replicate of an OU process with this slightly more complex model by modifying the code from question 1a. Store the times in `times` and the simulated phenotypes at each time in the variable `pheno`. Use the same parameter values for &theta; and &sigma; as you used in Question 1, and set &alpha; to equal 1.7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a605d407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6bd77aa",
   "metadata": {},
   "source": [
    "### Q2b.\n",
    "\n",
    "Define your negative log-likelihood function and the function you will use to evaluate the negative log-likelihood during optimization. Call these, respectively, `ornstein_ts_negll` and `evaluate_ornstein`. Remember that like &sigma;, &alpha; should not fall below or equal to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23461cf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "493ce0ed",
   "metadata": {},
   "source": [
    "### Q2c.\n",
    "\n",
    "Optimize values for all three parameters. Start with initial values &alpha; = 0.8, &theta; = 91, and &sigma; = 3. Print the estimates. Are they close to the true, simulated parameter values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbf5eb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a478ea09",
   "metadata": {},
   "source": [
    "### Q2d.\n",
    "\n",
    "Borrow and alter the code from lecture to visualize the 2-dimensional likelihood surface for &alpha; and &theta; using a contour plot. You might convert the negative log-likelihoods to positive log-likelihoods like we did in class. Label the axes. Is there a clearly defined peak in the contour plot? Does this mean there is strong or weak evidence from which to 'identify' which values for &alpha; and &theta; best explain the data?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b7dc4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06d6f473",
   "metadata": {},
   "source": [
    "### Q2e.\n",
    "\n",
    "Speculate on why the 2-dimensional likelihood surface in Q2d looks the way it does. What does this mean for statisitical estimation of our three-parameter OU model? Speculate on ways you might improve this estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e249bf4",
   "metadata": {},
   "source": [
    "WRITE ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
